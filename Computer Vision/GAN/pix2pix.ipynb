{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    kaggle = https://www.kaggle.com/code/leejin11/pix2pix-code\n",
    "'''"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# image2image GRAY2RGB 로 연습",
   "id": "ab47d7b1298cbede"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "de7b494cf6bd6cc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## network class",
   "id": "19f8160ee2261af2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class EncodingBlock(nn.Module):\n",
    "    '''EncodingBlock for G and D\n",
    "        Args:\n",
    "            in_dim(int) : input dimension\n",
    "            output(int) : output dimension\n",
    "    '''\n",
    "    def __init__(self, in_dim, out_dim, *, kernel_size=4, stride=2, padding=1, normalize=True):\n",
    "        super(EncodingBlock, self).__init__()\n",
    "        # 기본적인 conv2d layer 생성\n",
    "        layers = [nn.Conv2d(in_dim, out_dim, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)]\n",
    "\n",
    "        # normalize 여부에 따라 batchnorm2d 삽입\n",
    "        if normalize:\n",
    "            layers.append(nn.BatchNorm2d(out_dim))\n",
    "\n",
    "        # activation function 삽입\n",
    "        layers.append(nn.LeakyReLU(0.2))\n",
    "    \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "class DecodingBlock(nn.Module):\n",
    "    '''DecodingBlock for G\n",
    "        Args:\n",
    "            in_dim(int) : input dimension\n",
    "            output(int) : output dimension\n",
    "    '''\n",
    "    def __init__(self, in_dim, out_dim, *, kernel_size=4, stride=2, padding=1,dropout=False):\n",
    "        super(DecodingBlock, self).__init__()\n",
    "\n",
    "        # 모든 layer에 동일하게 적용\n",
    "        self.dropout = nn.Dropout2d(p=0.3) if dropout else nn.Identity()\n",
    "            \n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_dim, out_dim, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "            nn.ReLU(),\n",
    "            self.dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_input=None):\n",
    "        # 공간 정보(상위 feature)를 복원하기 위한 skip connetion 연결\n",
    "        if skip_input is not None:\n",
    "            x = torch.cat((x, skip_input), dim=1)\n",
    "            \n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "# Genrator\n",
    "class Generator(nn.Module):\n",
    "    '''Unet 기반 Generator model\n",
    "        Args:\n",
    "            in_dim(int) : Input dimension\n",
    "            out_dim(int) : output dimension\n",
    "            features(int) : hidden dimension\n",
    "    '''\n",
    "    def __init__(self, in_dim=3, out_dim=3, features=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.enc1 = EncodingBlock(in_dim, features, normalize=False)\n",
    "        self.enc2 = EncodingBlock(features, features*2)\n",
    "        self.enc3 = EncodingBlock(features*2, features*4)\n",
    "        self.enc4 = EncodingBlock(features*4, features*8)\n",
    "        self.enc5 = EncodingBlock(features*8, features*8)\n",
    "        self.enc6 = EncodingBlock(features*8, features*8, normalize=False)\n",
    "        \n",
    "        self.dec1 = DecodingBlock(features*8, features*8, dropout=True)\n",
    "        self.dec2 = DecodingBlock(features*16, features*8, dropout=True)\n",
    "        self.dec3 = DecodingBlock(features*16, features*4)\n",
    "        self.dec4 = DecodingBlock(features*8, features*2)\n",
    "        self.dec5 = DecodingBlock(features*4, features)\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features*2, out_dim, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input [b, c, h, w] = [b, 3, 64, 64]\n",
    "        e1 = self.enc1(x) # [b, 64, 32, 32]\n",
    "        e2 = self.enc2(e1) # [b, 128, 16, 16]\n",
    "        e3 = self.enc3(e2) # [b, 256, 8, 8]\n",
    "        e4 = self.enc4(e3) # [b, 512, 4, 4]\n",
    "        e5 = self.enc5(e4) # [b, 512, 2, 2]\n",
    "        e6 = self.enc6(e5) # [b, 512, 1, 1]\n",
    "        \n",
    "        d1 = self.dec1(e6) # [b, 512, 2, 2]\n",
    "        d2 = self.dec2(d1, e5) # [b, 1024(512 + 512), 2, 2] -> [b, 512, 4, 4]\n",
    "        d3 = self.dec3(d2, e4) # [b, 1024, 4, 4] -> [b, 256, 8, 8]\n",
    "        d4 = self.dec4(d3, e3) # [b, 512, 8, 8] -> [b, 128, 16, 16]\n",
    "        d5 = self.dec5(d4, e2) # [b, 256, 16, 16] -> [b, 64, 32, 32]\n",
    "        out = self.final(torch.cat((d5, e1), dim=1)) # [b, 128, 32, 32] -> [b, 3, 64, 64]\n",
    "        return out\n",
    "\n",
    "# Discrimnator\n",
    "class Discriminator(nn.Module):\n",
    "    '''Unet 기반 Discriminator model\n",
    "        Args:\n",
    "            in_dim(int) : Input dimension\n",
    "            out_dim(int) : output dimension\n",
    "            features(int) : hidden dimension\n",
    "    '''\n",
    "    def __init__(self, in_dim=3, out_dim=3,features=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # Input [b, 4, 64, 64]\n",
    "            EncodingBlock(in_dim + out_dim, features, normalize=False), # [b, 64, 32, 32]\n",
    "            EncodingBlock(features, features * 2), # [b, 128, 16, 16]\n",
    "            EncodingBlock(features * 2, features * 4), # [b, 256, 8, 8]\n",
    "            nn.Conv2d(features*4, out_dim, kernel_size=4, stride=2, padding=1), # [b, 3, 4, 4]\n",
    "        )\n",
    "        \n",
    "    def forward(self, img_A, img_B):\n",
    "        # input [b, 1, h, w], [b, 3, h, w] = [b, 1, 64, 64], [b, 3, 64, 64]\n",
    "        x = torch.cat((img_A, img_B), dim=1) # [b, 4, 64, 64] \n",
    "        x = self.model(x) # [b, 3, 4, 4]\n",
    "        return x"
   ],
   "id": "f0a39c82eabb4520"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Loading Data",
   "id": "478a33b3e5a65628"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print('loading data')\n",
    "img_list = list()\n",
    "\n",
    "image_path = '/kaggle/input/imagenet1k0'\n",
    "for cls_name in os.listdir(image_path):\n",
    "    for img_name in os.listdir(os.path.join(image_path, cls_name)):\n",
    "        # 이미지의 이름을 저장\n",
    "        img_list.append(os.path.join(cls_name, img_name))\n",
    "print('finised loading data')\n",
    "\n",
    "img_list = img_list[:50_000]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device type : {device}\\n')\n",
    "\n",
    "## dataloader <- 이미지를 ram에 올리는 것은 불가능.\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_list, size=256):\n",
    "        self.img_list = img_list\n",
    "        self.path = image_path\n",
    "        self.size = size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_path = os.path.join(self.path, self.img_list[idx])\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        # gray와 color (Input 과 Output)\n",
    "        img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        img_color = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # 이미지의 크기를 size로 조절\n",
    "        img_gray = cv2.resize(img_gray, (self.size, self.size))        \n",
    "        img_color = cv2.resize(img_color, (self.size, self.size))\n",
    "\n",
    "        # normalization\n",
    "        img_gray = img_gray.astype(np.float32) / 255.0 * 2 - 1\n",
    "        img_color = img_color.astype(np.float32) / 255.0 * 2 - 1\n",
    "\n",
    "        # tensor 형태로 변환\n",
    "        img_gray = torch.from_numpy(img_gray)\n",
    "        img_gray = img_gray.unsqueeze(-1).permute(2, 0, 1) # [h, w]의 형태를 [1, h, w]로 변환\n",
    "        img_color = torch.from_numpy(img_color).permute(2, 0, 1)\n",
    "\n",
    "        return img_gray, img_color"
   ],
   "id": "efd1271ae763f18c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# train",
   "id": "bec33e064e9ed0d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "batch_size = 1\n",
    "lr = 2e-4\n",
    "epochs = 10\n",
    "betas = (0.5, 0.999)\n",
    "gamma = 100\n",
    "size = 64\n",
    "\n",
    "dataset = CustomDataset(img_list, size)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "g_model = Generator(in_dim=1).to(device)\n",
    "d_model = Discriminator(in_dim=1).to(device)\n",
    "\n",
    "g_optimizer = torch.optim.Adam(g_model.parameters(), lr=lr, betas=betas)\n",
    "d_optimizer = torch.optim.Adam(d_model.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "criterion_GAN = nn.BCEWithLogitsLoss()\n",
    "criterion_L1 = nn.L1Loss()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for e in range(epochs):\n",
    "    for i, (gray, color) in enumerate(dataloader):\n",
    "        gray, color = gray.to(device), color.to(device)\n",
    "    \n",
    "        ## generator\n",
    "        fake_img = g_model(gray) # 새로운 이미지 생성\n",
    "        pred_fake = d_model(gray, fake_img) # discriminator가 인풋과 생성 이미지를 판단\n",
    "        \n",
    "        valid = torch.ones_like(pred_fake)\n",
    "        fake  = torch.zeros_like(pred_fake)\n",
    "        \n",
    "        loss_GAN = criterion_GAN(pred_fake, valid) # discriminator를 속이는 loss\n",
    "        loss_L1 = criterion_L1(fake_img, color) # 생성 이미지의 유사도 loss\n",
    "        \n",
    "        g_loss = loss_GAN + (gamma * loss_L1) # 이미지의 유사도의 gamma 가중치 적용\n",
    "        g_loss.backward()\n",
    "    \n",
    "        g_optimizer.zero_grad()\n",
    "        g_optimizer.step()\n",
    "    \n",
    "        ## discriminator\n",
    "        pred_real = d_model(gray, color) # discrimonator가 Input을 진짜라고 판단\n",
    "        pred_fake = d_model(gray, fake_img.detach()) # discriminator가 Input을 가짜라고 판단\n",
    "        \n",
    "        d_loss_real = criterion_GAN(pred_real, valid)\n",
    "        d_loss_fake = criterion_GAN(pred_fake, fake)\n",
    "        \n",
    "        d_loss = 0.5 * (d_loss_real + d_loss_fake) # 두 개의 로스를 적용\n",
    "        d_loss.backward()\n",
    "    \n",
    "        d_optimizer.zero_grad()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        info = f'epoch : {e}    iter : {i+1:5d}    d_loss : {d_loss:.4f}  g_loss : {g_loss:.4f}'\n",
    "        with open('output.txt', 'a') as f:\n",
    "            f.write(info + '\\n')\n",
    "        \n",
    "        \n",
    "        if (i+1) % 1000 == 0:\n",
    "            print(info +f'     time : {time.time()-start_time:5.3f}')\n",
    "            start_time = time.time()    \n",
    "    # 이미지 저장 및 시각화 코드\n",
    "    save_image([gray.repeat(1, 3, 1, 1)[0], fake_img[0], color[0]], f'output{e}.jpg', nrow=3, normalize=True)\n",
    "    img = cv2.imread(f'output{e}.jpg')\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "        \n",
    "torch.save({\n",
    "    'discriminator': d_model.state_dict(),\n",
    "    'd_optimizer': d_optimizer.state_dict(),\n",
    "    'generator': g_model.state_dict(),\n",
    "    'g_optimizer': g_optimizer.state_dict(),\n",
    "    'epoch': epochs,\n",
    "    \n",
    "}, 'checkpoint.pth')"
   ],
   "id": "516da745717e308"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
